#!/usr/bin/env python
# coding: utf-8
# This is a demo, given a T-LESS image 0001.png, it could do 2D detection with RetinaNet, and do 6D pose estimation then for the box model.
# To run it: python single3_test_demo.py
#Prerequisite:
#ckpt: under path_workspace/experiments/<experiment_name>/checkpoints_lambda250/checkpoints/ckpt-<num_iterations>-1
#Info of reference rotations \bar_R(Generated by render_codebook.py): regarding 2D boundingbox, rotation, and codebook, which are saved under the path: path_embedding_data
#The trained 2D detector ReninaNet, and image.

use_retinanet=False
if use_retinanet:
	import keras
	from keras_retinanet import models
	from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image

import cv2
import os
import numpy as np
import time
import tensorflow as tf
import sonnet as snt

obj_id=29
num_iterations=30000
LATENT_SPACE_SIZE = 128
NUM_FILTER = [128, 256, 512, 512]
KERNEL_SIZE_ENCODER = 5
STRIDES =[2, 2, 2, 2]
BATCH_NORM = False
image_size=128
embedding_dim = 128
num_embeddings = 92232 # embedd画像の枚数 
K_train = np.array([1075.65, 0, 720 / 2, 0, 1073.90, 540 / 2, 0, 0, 1]).reshape(3, 3) # 学習時のカメラ行列.Should be consistent with \bar_R images
Radius_render_train = 700 # 焦点距離

K_test = np.array([1075.65091572, 0.0, 374.06888344, 0.0, 1073.90347929, 255.72159802, 0.0, 0.0, 1.0]).reshape(3,3) # テスト時のカメラ行列
experiment_name='subdiv_{:02d}_softmax_edge'.format(obj_id) 
path_workspath='./ws/'
path_embedding_data='./embedding92232s/{:02d}'.format(obj_id) # embedd画像の入ったディレクトリ #path to dir of info \bar_R

#### Step 0: Load pose estimation network
class Encoder(snt.AbstractModule):
    def __init__(self, latent_space_size, num_filters, kernel_size, strides, batch_norm, name='encoder'):
        super(Encoder, self).__init__(name=name)
        self._latent_space_size = latent_space_size
        self._num_filters = num_filters
        self._kernel_size = kernel_size
        self._strides = strides
        self._batch_normalization = batch_norm

    @property
    def latent_space_size(self):
        return self._latent_space_size

    @property
    def encoder_layers(self):
        layers = []
        x = self._input
        layers.append(x)
        for filters, stride in zip(self._num_filters, self._strides):
            padding = 'same'
            x = tf.layers.conv2d(
                inputs=x,
                filters=filters,
                kernel_size=self._kernel_size,
                strides=stride,
                padding=padding,
                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),
                activation=tf.nn.relu,
            )
            if self._batch_normalization:
                x = tf.layers.batch_normalization(x, training=self._is_training)
            layers.append(x)
        return layers

    @property
    def encoder_out(self):
        x = self.encoder_layers[-1]
        encoder_out = tf.contrib.layers.flatten(x)

        return encoder_out

    @property
    def z(self):
        x = self.encoder_out
        # construct data
        z = tf.layers.dense(
            x,
            self._latent_space_size,
            activation=None,
            kernel_initializer=tf.contrib.layers.xavier_initializer(),
            name=None
        )
        return z

    def _build(self, x, is_training=False):
        self._input = x
        self._is_training = is_training
        return self.z


class VectorQuantizer(snt.AbstractModule): # ベクトル量子化器
    def __init__(self, embedding_dim, num_embeddings, name='vq_center'):
        super(VectorQuantizer, self).__init__(name=name)
        self._embedding_dim = embedding_dim
        self._num_embeddings = num_embeddings

        with self._enter_variable_scope():
            initializer = tf.uniform_unit_scaling_initializer()
            self._w = tf.get_variable('embedding', [embedding_dim, num_embeddings], initializer=initializer, trainable=True)

    def _build(self, inputs):
        input_shape = tf.shape(inputs)
        with tf.control_dependencies([
            tf.Assert(tf.equal(input_shape[-1], self._embedding_dim),[input_shape])]):
            flat_inputs = tf.reshape(inputs, [-1, self._embedding_dim])
            w = self.embeddings.read_value()

        distances = -tf.matmul(tf.nn.l2_normalize(flat_inputs, axis=1), tf.nn.l2_normalize(w, axis=0))
        encoding_indices = tf.argmax(- distances, 1)
        encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])        
        return {'encoding_indices': encoding_indices, }

    @property
    def embeddings(self):
        return self._w


# Build modules.
graph_estpose=tf.Graph()
with graph_estpose.as_default():
    with tf.variable_scope(experiment_name):
        I_x = tf.placeholder(tf.float32, shape=(None, image_size, image_size, 4))
        with tf.variable_scope('encoder'):
            encoder = Encoder(latent_space_size=LATENT_SPACE_SIZE,num_filters=NUM_FILTER,kernel_size=KERNEL_SIZE_ENCODER,strides=STRIDES,batch_norm=BATCH_NORM)
        z = encoder(I_x)
        network_vars = tf.trainable_variables()
        print(network_vars)
        vq_codebook = VectorQuantizer(embedding_dim=embedding_dim,num_embeddings=num_embeddings)

        # For evaluation, make sure is_training=False!
        with tf.variable_scope('validation'):
            nn_item = vq_codebook(z)

        # Bounding box informations for foreground model in pose template repository
        codebook_obj_bbs = np.load(os.path.join(path_embedding_data,'obj_bbs.npy'))
        codebook_rotations = np.load(os.path.join(path_embedding_data,'rot_infos.npz'))['rots']

    saver = tf.train.Saver(network_vars, save_relative_paths=False)
    embedding = tf.placeholder(tf.float32, shape=[embedding_dim, num_embeddings])
    embedding_assign_op = tf.assign(vq_codebook.embeddings, embedding)


gpu_options = tf.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.9)
config = tf.ConfigProto(gpu_options=gpu_options)
sess_estpose=tf.Session(graph=graph_estpose,config=config)
print('Step 0, Load Rotation Estimation Net')
with sess_estpose.as_default():
    with graph_estpose.as_default():
        saver.restore(sess_estpose, '{:s}/experiments/{:s}/checkpoints_lambda250/checkpoints/chkpt-{:d}'.format(path_workspath,experiment_name,num_iterations-1))
        arr_codebook = np.load(os.path.join(path_embedding_data,'edgeLambda250_codebook.npy'))
        sess_estpose.run(embedding_assign_op, {embedding: arr_codebook.T})

if use_retinanet: # retinanetを使う場合
	#### Step 0: Load 2D detection network (Retina)
	print('Step 0, Load Retina Net')
	graph_detect=tf.Graph()
	sess_detect=tf.Session(graph=graph_detect,config=config)
	with sess_detect.as_default():
	    with graph_detect.as_default():
	        detection_model_path = './demo_data/resnet50_tless_19_inf.h5'
	        detection_model = models.load_model(detection_model_path, backbone_name='resnet50')

	print('Step 0, Load RGB image')
	image = cv2.imread('./demo_data/0001.png') # テスト画像:0001.pngの読み込み->全部行くならfor文で回せばおけ

	#### Step 1: 2D detection 
	print('Step 1, 2D detection')
	with sess_detect.as_default():
	    with graph_detect.as_default():
	        detect_image = preprocess_image(image.copy())
	        detect_image, scale = resize_image(detect_image)

	        # process image
	        start = time.time()
	        boxes, scores, labels = detection_model.predict_on_batch(np.expand_dims(detect_image, axis=0))

	        # correct for image scale
	        boxes /= scale # boxes = boxes / scale = boxes / sclae <- ressize_imageから（faster-aae-tless/codebook_render/boxes.py） 

	        obj_bb=None
	        # visualize detections
	        for box, score, label in zip(boxes[0], scores[0], labels[0]):        
	            if score<0.:
	            	break
	            if label+1==obj_id: # obj_idは自分で決める
	            	obj_bb=np.array(box).astype(int) # (x1,y1,x2,y2)
	            	obj_bb[2]-=obj_bb[0] # obj_bb[2] = obj_bb[2] - obj_bb[0] = x2-x1 = width
	            	obj_bb[3]-=obj_bb[1] # obj_bb[3] = obj_bb[3] - obj_bb[1] = y2-y1 = height
			# obj_bb:(x1,y1,x2,y2)-> (x1,y1,width,height)
	            	break
	if obj_bb is None: # 検出器のbbがNoneならば
	    print('No valid detection') # 有効な検出無し
	    exit(0) # 終了


else: # retinanetを使わない場合
	print('Step 0, Load RGB image')
	image = cv2.imread('./demo_data/0001.png')

	#### Step 1: 2D detection 
	print('Step 1, 2D detection')
	obj_bb=np.array([352,112,176,167],dtype=np.int32) # 検出器の情報ではなく，GTのBBox情報を自分で与えている


#obj_bb is the 2D bounding box on the image, with 4 elements: x,y,w,h; where (x,y) is the location of the left-top corner
#Thus center 2D location is (x+w/2, h+w/2) <- ミスやと思われ
# obj_bbは，画像上の2次元バウンディングボックスで、x,y,w,hの4要素を持ちます；(x,y)は左上隅の位置です
# センターの2D位置は(x+w/2, y+h/2)となります。-> cx = x+w/2, cy = y+h/2
verbose=False
if verbose: # 検出領域を表示するか否か
    image_vis=image.copy()
    cv2.rectangle(image_vis,(obj_bb[0],obj_bb[1]),(obj_bb[0]+obj_bb[2],obj_bb[1]+obj_bb[3]),(255,0,0),2)
    # -> cv2.rectangle(画像，(左上の座標：x1=x,y1=y), (右下の座標：x2=x+w,y2=y+h), color:(B,G,R)=(255,0,0)より青線, 線の太さ：2pix)
    cv2.imshow('img',image_vis)
    cv2.waitKey()

print('Step 2, Pose estimation')
with sess_estpose.as_default():
    with graph_estpose.as_default():
        img_bgr=image.copy()

        x,y,w,h=obj_bb
        H,W,_=img_bgr.shape
        size = int(np.maximum(h, w) * 1.2)
        left = int(np.max([x + w // 2 - size // 2, 0]))
        right = int(np.min([x + w // 2 + size // 2, W]))
        top = int(np.max([y + h // 2 - size // 2, 0]))
        bottom = int(np.min([y + h // 2 + size // 2, H]))

        crop = img_bgr[top:bottom, left:right].copy() # 画像からはみ出ないサイズにBBoxを拡張して，RGB画像のその領域のみをcropとして保持
        query_bgr = cv2.resize(crop, (image_size,image_size)) # RGB画像（対象物体領域）を128x128へリサイズ
        query_edge = np.expand_dims(cv2.Canny(query_bgr, 50, 150),2) # Canny法でEdge化
        query = np.expand_dims((np.concatenate((query_bgr, query_edge), axis=-1) /255.),0) # 対象物体のRGB画像とEdge画像をconcatし，queryとする

        idx=sess_estpose.run([nn_item],feed_dict={I_x:query})
        idx=idx[0]['encoding_indices'][0]
        est_rot_cb= codebook_rotations[idx] #This should be a 3x3 matrix, which indicates an initial model-to-camera rotation estimation

        if verbose:
            #path='../Edge-Network/embedding92232s/{:02d}/imgs/{:05d}.png'.format(obj_id,idx)
            #est_bgr=cv2.imread(path)
            cv2.imshow('bbox',query_bgr)
            #cv2.imshow('estimated rotation',est_bgr)
            cv2.waitKey()

        est_translation=True
        if est_translation:#If we do not know the depth, and it is required to be calculated. Depth情報が不明な場合：RGBのみで姿勢推定する場合
            K00_ratio = K_test[0, 0] / K_train[0, 0]
            K11_ratio = K_test[1, 1] / K_train[1, 1]

            mean_K_ratio = np.mean([K00_ratio, K11_ratio])

            render_bb = codebook_obj_bbs[idx].squeeze()
            est_bb = obj_bb.copy() #Same as obj_bb, thus est_bb=[x,y,w,h], is the 2D bounding box detected, where (x,y) is the 2D location of left-top corner
            #Center of the 2D bbox is (x_c,y_c)=(est_bb[0]+est_bb[2]/2,est_bb[1]+est_bb[3]/2)
            diag_bb_ratio = np.linalg.norm(np.float32(render_bb[2:])) / np.linalg.norm(np.float32(est_bb[2:]))

            mm_tz = diag_bb_ratio * mean_K_ratio * Radius_render_train
        else:#If we could directly know the depth, like the distance between camera and table
            mm_tz=748


        center_obj_x_train = render_bb[0] + render_bb[2] / 2. - K_train[0, 2]
        center_obj_y_train = render_bb[1] + render_bb[3] / 2. - K_train[1, 2]

        center_obj_x_test = est_bb[0] + est_bb[2] // 2 - K_test[0, 2]
        center_obj_y_test = est_bb[1] + est_bb[3] // 2 - K_test[1, 2]

        center_obj_mm_tx = center_obj_x_test * mm_tz / K_test[0, 0] \
                           - center_obj_x_train * Radius_render_train / K_train[0, 0]
        center_obj_mm_ty = center_obj_y_test * mm_tz / K_test[1, 1] \
                           - center_obj_y_train * Radius_render_train / K_train[1, 1]
        est_tra = np.array([center_obj_mm_tx, center_obj_mm_ty, mm_tz]).reshape((3, 1)) ####This is the model-to-camera translation estimation, unit is mm


        #Ractify
        d_alpha_x = - np.arctan(est_tra[0] / est_tra[2])
        d_alpha_y = - np.arctan(est_tra[1] / est_tra[2])
        R_corr_x = np.array([[1, 0, 0],
                             [0, np.cos(d_alpha_y), -np.sin(d_alpha_y)],
                             [0, np.sin(d_alpha_y), np.cos(d_alpha_y)]])
        R_corr_y = np.array([[np.cos(d_alpha_x)[0], 0, -np.sin(d_alpha_x)[0]],
                             [0, 1, 0],
                             [np.sin(d_alpha_x), 0, np.cos(d_alpha_x)]])
        est_rot = np.dot(R_corr_y, np.dot(R_corr_x, est_rot_cb))#### his is to correct the rotation estimaton with regard to the perspective error
        #####est_rot is the final model-to-camera rotation estimation, 3x3 rotation estimation

print('Step 3: Write result')
print('Estimated rotation - model2cam, 3x3 rotation matrix:') ###3x3 rotation matrix
print(est_rot.astype(np.float32).reshape((3,3)))
print('Estimated translation - model2cam, in mm:') ###translation vector, with unit mm
print(est_tra.astype(np.float32).reshape((1,3)))
print('Detected 2D bounding box - on 2D image, (x_topleft,y_topleft,bbox_width,bbox_height:')
print(np.array(obj_bb).astype(np.int32).reshape((1,4))) ### 2D bounding box


path_result='./result.txt'
with open(path_result, 'w') as f:
    line_tpl=', '.join(['{:.8f}'] * 9) + '\n' + ', '.join(['{:.8f}'] * 3)
    Rt = est_rot.astype(np.float32).flatten().tolist() + est_tra.flatten().tolist()
    txt=line_tpl.format(*Rt)
    f.write(txt)

'''
Expected result:
Estimated rotation:
[[ 0.86691886 -0.00222524  0.4984443 ]
 [ 0.49393854  0.13806163 -0.8584659 ]
 [-0.06690574  0.9904211   0.12078737]]
Estimated translation:
[[ 45.84993 -40.99217 748.031  ]]

'''
